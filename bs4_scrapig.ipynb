{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://keithgalli.github.io/web-scraping/example.html\")\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "soup = bs(r.content)\n",
    "\n",
    "# Print out our html\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"paragraph-id\"><b>Some bold text</b></p>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(['h2', 'h1'])\n",
    "soup.find_all('p', attrs={'id':'paragraph-id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h2>Another header</h2>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "soup.find_all('h2', string = re.compile('header'))\n",
    "# soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>Some bold text</b>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"p#paragraph-id b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select paragrph of class yahia\n",
    "soup.select('p.yahia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://keithgalli.github.io/web-scraping/\"\n",
    "r = requests.get(base_url+\"webpage.html\")\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "soup = bs(r.content)\n",
    "\n",
    "# Print out our html\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task-1 grap all social media links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram: \n",
      "https://www.instagram.com/keithgalli/\n",
      "Twitter: \n",
      "https://twitter.com/keithgalli\n",
      "LinkedIn: \n",
      "https://www.linkedin.com/in/keithgalli/\n",
      "TikTok: \n",
      "https://www.tiktok.com/@keithgalli\n"
     ]
    }
   ],
   "source": [
    "# grap all of social links - soup method\n",
    "# links = soup.find_all('li', attrs='social')   # correct also\n",
    "links = soup.find_all('li', attrs={'class':'social'})\n",
    "# print (links)\n",
    "for l in links:\n",
    "    print (l.find('b').string)\n",
    "    print (l.find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instagram: \n",
      "https://www.instagram.com/keithgalli/\n",
      "Twitter: \n",
      "https://twitter.com/keithgalli\n",
      "LinkedIn: \n",
      "https://www.linkedin.com/in/keithgalli/\n",
      "TikTok: \n",
      "https://www.tiktok.com/@keithgalli\n"
     ]
    }
   ],
   "source": [
    "# grap all of social links - CSS select method\n",
    "links = soup.select('ul.socials li')\n",
    "# print (links)\n",
    "for l in links:\n",
    "    print (l.select('b')[0].string)\n",
    "    print (l.select('a[href]')[0].string) # attribute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task-2: grap table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'Team', 'League', 'GP', 'G', 'A', 'TP', 'PIM', '+/-', '', 'POST', 'GP', 'G', 'A', 'TP', 'PIM', '+/-']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>Team</th>\n",
       "      <th>League</th>\n",
       "      <th>GP</th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>TP</th>\n",
       "      <th>PIM</th>\n",
       "      <th>+/-</th>\n",
       "      <th></th>\n",
       "      <th>POST</th>\n",
       "      <th>GP</th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>TP</th>\n",
       "      <th>PIM</th>\n",
       "      <th>+/-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-15</td>\n",
       "      <td>MIT (Mass. Inst. of Tech.)</td>\n",
       "      <td>ACHA II</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "      <td>|</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-16</td>\n",
       "      <td>MIT (Mass. Inst. of Tech.)</td>\n",
       "      <td>ACHA II</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>|</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-17</td>\n",
       "      <td>MIT (Mass. Inst. of Tech.)</td>\n",
       "      <td>ACHA II</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>|</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-18</td>\n",
       "      <td>Did not play</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>|</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-19</td>\n",
       "      <td>MIT (Mass. Inst. of Tech.)</td>\n",
       "      <td>ACHA III</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>|</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         S                        Team    League  GP  G   A  TP PIM +/-     \\\n",
       "0  2014-15  MIT (Mass. Inst. of Tech.)   ACHA II  17  3   9  12  20      |   \n",
       "1  2015-16  MIT (Mass. Inst. of Tech.)   ACHA II   9  1   1   2   2      |   \n",
       "2  2016-17  MIT (Mass. Inst. of Tech.)   ACHA II  12  5   5  10   8   0  |   \n",
       "3  2017-18                Did not play                                   |   \n",
       "4  2018-19  MIT (Mass. Inst. of Tech.)  ACHA III   8  5  10  15   8      |   \n",
       "\n",
       "  POST GP G A TP PIM +/-  \n",
       "0                         \n",
       "1                         \n",
       "2                         \n",
       "3                         \n",
       "4                         "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrap table headers - Use find/find_all\n",
    "import pandas as pd\n",
    "\n",
    "hs = soup.find_all('th')\n",
    "\n",
    "headers = [str(h.string).strip() for h in hs]\n",
    "print(headers)\n",
    "\n",
    "rows = soup.find('tbody').find_all('tr')\n",
    "# rows = soup.find_all(class_='team-continent-NA')\n",
    "data = []\n",
    "rows\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    row = [str(c.get_text()).strip() for c in cols]\n",
    "    data.append(row)\n",
    "\n",
    "\n",
    "# print (data)\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap table data - use select\n",
    "\n",
    "\n",
    "rows = soup.select('tr.team-continent-NA')\n",
    "rows\n",
    "for r in rows:\n",
    "    # r.select('td')\n",
    "    # print ('row-->', type(r), '\\n', r )\n",
    "    # for i, c in enumerate(r):\n",
    "    #     print ('-->', c.select('td'))\n",
    "        # print (i,'-->', c)\n",
    "    season = r.select('td.season')[0].string\n",
    "    # season = r.find_all('td.season')[0]\n",
    "    team = r.select('td.team a')[0].string\n",
    "    league = r.select('td.league a')[0].string\n",
    "\n",
    "    # gps = \n",
    "    # for i,p in enumerate(gps):\n",
    "    gps =  [str(p.string).strip() for p in r.select('td.regular')]\n",
    "    \n",
    "    # post_gps = r.select('td.postseason')\n",
    "    post_gps = [str(p.string).strip() for p in r.select('td.postseason') ]\n",
    "    # for i,p in enumerate(post_gps):\n",
    "    #     print (i, '-', p.string)\n",
    "    print (str(season).strip(' \\n'), team, league, gps, post_gps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task-3 grap all fun facts that include word 'is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Middle name is Ronald\n",
      "Dunkin Donuts coffee is better than Starbucks\n",
      "A favorite book series of mine is Ender's Game\n",
      "Current video game of choice is Rocket League\n",
      "The band that I've seen the most times live is the Zac Brown Band\n"
     ]
    }
   ],
   "source": [
    "# merged select with find\n",
    "facts = soup.select('ul.fun-facts li')  # tag ul with class = fun-facts\n",
    "\n",
    "for f in facts:\n",
    "    fs = f.get_text()\n",
    "    if 'is' in fs:\n",
    "        print(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task-4: scrap photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake Como\n",
      "Pontevecchio, Florence\n",
      "Riomaggiore, Cinque de Terre\n"
     ]
    }
   ],
   "source": [
    "imgs = soup.select('div.row img')\n",
    "for img in imgs:\n",
    "    img_url = base_url + img.attrs['src']\n",
    "    image_name = img['alt']\n",
    "    print (image_name)\n",
    "    image_content = requests.get(img_url).content\n",
    "    with open (os.path.join('.','out', image_name + '.jpg'), 'wb') as fp:\n",
    "        fp.write(image_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task-5: secret word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Make sure to smash that like button and subscribe !!!\n"
     ]
    }
   ],
   "source": [
    "links = soup.select('div.block li') # #secret-word\n",
    "words=\"\"\n",
    "for lnk in links:\n",
    "    # print (lnk)\n",
    "    file_url = lnk.find('a')['href']\n",
    "    # print (file_url)\n",
    "    file_content = bs(requests.get(base_url+file_url).content)\n",
    "    secret_words=file_content.find('p',id='secret-word').string\n",
    "    words = words + \" \" + str(secret_words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bs4 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = soup.find_all('a')\n",
    "\n",
    "# for y in x:\n",
    "#     print (y.get('href'), y.get_text())\n",
    "\n",
    "\n",
    "# soup.p['class']\n",
    "soup.find_all('p')\n",
    "\n",
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul class=\"socials\">\n",
       " <li class=\"social instagram\"><b>Instagram: </b><a href=\"https://www.instagram.com/keithgalli/\">https://www.instagram.com/keithgalli/</a></li>\n",
       " <li class=\"social twitter\"><b>Twitter: </b><a href=\"https://twitter.com/keithgalli\">https://twitter.com/keithgalli</a></li>\n",
       " <li class=\"social linkedin\"><b>LinkedIn: </b><a href=\"https://www.linkedin.com/in/keithgalli/\">https://www.linkedin.com/in/keithgalli/</a></li>\n",
       " <li class=\"social tiktok\"><b>TikTok: </b><a href=\"https://www.tiktok.com/@keithgalli\">https://www.tiktok.com/@keithgalli</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = soup.find_all('a')\n",
    "\n",
    "# for y in x:\n",
    "#     print (y.get('href'), y.get_text())\n",
    "\n",
    "\n",
    "# soup.p['class']\n",
    "soup.find_all('ul')\n",
    "\n",
    "# print (soup.prettify())\n",
    "soup.ul['class']\n",
    "soup.find_all('ul',attrs={'class':\"socials\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "tag = soup.h1\n",
    "type(tag)\n",
    "tag.name\n",
    "tag = soup.a\n",
    "# print (tag, type(tag))\n",
    "tag['href']\n",
    "tag.string\n",
    "tag.attrs\n",
    "soup.p\n",
    "soup.find('p')  # same as above\n",
    "soup.find_all('p', attrs={'id':'footer'})\n",
    "\n",
    "soup.body.p.contents\n",
    "soup.contents[0].name\n",
    "\n",
    "# for i,x in enumerate(soup.children):\n",
    "#     print (i, '------->', x)\n",
    "\n",
    "for x in soup.stripped_strings:\n",
    "    x\n",
    "\n",
    "x = soup.a\n",
    "x.next_element\n",
    "x.next_element\n",
    "for i in range(10):\n",
    "    # print (i, '--', x.next_elements)\n",
    "    pass\n",
    "\n",
    "#regular expression\n",
    "import re\n",
    "\n",
    "#This code finds all the tags whose names contain the letter ‘t’:\n",
    "for tag in soup.find_all(re.compile(\"t\")):\n",
    "    # print(tag.name)\n",
    "    pass\n",
    "\n",
    "# find a or b\n",
    "soup.find_all([\"a\", \"p\"])\n",
    "\n",
    "# value True maches every tag\n",
    "for tag in soup.find_all(True, limit=2):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Funtion in find/_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_class_but_no_id(tag):\n",
    "    return tag.has_attr('class') and not tag.has_attr('id')\n",
    "\n",
    "soup.find_all(has_class_but_no_id)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
